---
license: apache-2.0
language:
  - ja
  - en
library_name: transformers
pipeline_tag: image-text-to-text
tags:
  - vision
  - vlm
  - qwen
  - lora
  - document-understanding
  - form-detection
  - japanese
base_model: Qwen/Qwen3-VL-8B-Instruct
datasets:
  - hand-dot/pdfme-form-field-dataset
---

# PDFme Form Field Detector

**Detects form fields that applicants need to fill in Japanese documents.**

This model is fine-tuned from [Qwen3-VL-8B-Instruct](https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct) using QLoRA to detect input fields in Japanese application forms, registration documents, and other official paperwork.

## What This Model Does

Given an image of a Japanese document, this model identifies the bounding boxes of form fields that **applicants/customers** should fill in, while **excluding fields meant for staff/officials**.

### Example Use Cases

- Automating form digitization
- Building PDF form generators
- Creating accessibility tools for document processing

## Model Details

| Item | Value |
|------|-------|
| Base Model | Qwen/Qwen3-VL-8B-Instruct |
| Fine-tuning Method | QLoRA (4-bit quantization + LoRA) |
| Training Data | [hand-dot/pdfme-form-field-dataset](https://huggingface.co/datasets/hand-dot/pdfme-form-field-dataset) |
| Output Format | JSON with normalized bbox coordinates (0-1000) |

## Performance

### Training Progress

| Epoch | Loss | Learning Rate |
|-------|------|---------------|
| 0.4 | 20.74 | 0 |
| 1.0 | 20.84 | 0.000175 |
| 2.0 | 15.05 | 0.0001 |
| 2.8 | 12.24 | 0.00005 |

**Loss improved: 20.74 â†’ 12.24 (41% reduction)**

### Current Limitations

1. **Small training dataset (10 samples)** - Limited generalization
2. **Bbox coordinate precision** - Depends on image size due to normalization
3. **Complex layouts** - May miss fields in multi-column or complex documents

### Evaluation Results

Evaluated on 10 training samples:

| Metric | Value | Description |
|--------|-------|-------------|
| **Recall** | 0% | Matched predictions with IoUâ‰¥0.5 |
| **Precision** | 0% | Correct predictions with IoUâ‰¥0.5 |
| **Average IoU** | 0.08 | Overlap between predicted and ground truth |

âš ï¸ **Current Status**: With only 10 training samples, the model can detect the "presence" of form fields but **location accuracy is low**. The output format (bbox_2d) is learned correctly, but more data is needed to improve coordinate precision.

### Metric Definitions

- **Recall**: Percentage of ground truth fields detected with IoUâ‰¥0.5
- **Precision**: Percentage of detected fields that are correct with IoUâ‰¥0.5
- **IoU**: Intersection over Union - overlap between predicted and ground truth bboxes (0-1)

## Quick Start

### Installation

```bash
pip install transformers peft torch accelerate bitsandbytes
```

### Inference

```python
import torch
from PIL import Image
from transformers import AutoProcessor, AutoModelForImageTextToText
from peft import PeftModel

# Load model
base_model = "Qwen/Qwen3-VL-8B-Instruct"
model = AutoModelForImageTextToText.from_pretrained(
    base_model,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True,
)
model = PeftModel.from_pretrained(model, "takumi123xxx/pdfme-form-field-detector-lora")
processor = AutoProcessor.from_pretrained(base_model, trust_remote_code=True)

# Prepare prompt
system_prompt = """You are an expert at analyzing Japanese documents.
There are two types of input fields:
1. Fields for applicants/customers to fill â†’ Target for detection
2. Fields for staff/officials to fill â†’ Exclude from detection"""

user_prompt = """Detect all input fields that applicants should fill in this image.
Exclude fields for staff.
Return JSON with bbox coordinates (0-1000 normalized)."""

# Load image
image = Image.open("your_document.png").convert("RGB")

messages = [
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": [
        {"type": "image", "image": image},
        {"type": "text", "text": user_prompt},
    ]},
]

text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
inputs = processor(text=text, images=image, return_tensors="pt").to(model.device)

output = model.generate(**inputs, max_new_tokens=2048)
result = processor.decode(output[0], skip_special_tokens=True)
print(result)
```

### Output Format

```json
{
  "applicant_fields": [
    {"bbox": [100, 200, 500, 250]},
    {"bbox": [100, 300, 500, 350]}
  ],
  "count": 2
}
```

- `bbox`: `[x1, y1, x2, y2]` normalized to 0-1000 scale
- To convert to pixels: `pixel_x = bbox_x / 1000 * image_width`

## Future Improvements

### Short-term

1. **Data augmentation** - Rotation, scaling, noise to expand training data
2. **Hyperparameter tuning** - Increase epochs, adjust learning rate
3. **Evaluation pipeline** - Automated IoU, Precision, Recall calculation

### Mid-term

4. **Field type classification** - Identify field types (name, address, date, etc.)
5. **Multi-turn dialogue** - Support conditional detection ("only detect name fields")
6. **Model ensemble** - Train multiple LoRA adapters and vote

### Long-term

7. **Large-scale dataset** - 1000+ annotated samples across document types
8. **Larger models** - Qwen3-VL-72B when PEFT compatible
9. **Active learning** - Human review â†’ feedback â†’ continuous improvement

## Deployment (Inference Endpoints)

### âš ï¸ Important: Instance Selection

This model is an **8B parameter** Vision-Language Model. Please note the following when deploying:

| Condition | Recommended Instance | VRAM | Notes |
|-----------|---------------------|------|-------|
| **With 4-bit quantization** | `nvidia-l4` or `nvidia-a10g` | 24GB | â­ Recommended |
| **Without 4-bit quantization** | `nvidia-a10g` or higher | 24GB+ | Requires more VRAM |

**ğŸ¯ Recommended: `nvidia-l4` Ã— 1 (with 4-bit quantization)**

### Deployment Steps

1. Go to [takumi123xxx/pdfme-form-field-detector-lora](https://huggingface.co/takumi123xxx/pdfme-form-field-detector-lora)
2. Click **"Deploy" â†’ "Inference Endpoints"**
3. Configure settings:

| Setting | Value | Description |
|---------|-------|-------------|
| **Cloud Provider** | `AWS` | Recommended |
| **Region** | `us-east-1` or `eu-west-1` | L4 available regions |
| **Instance Type** | â­ **`nvidia-l4`** | Best choice |
| **Instance Size** | `x1` | 1 GPU |
| **Min Replicas** | `0` | No charge when idle |
| **Max Replicas** | `1` | Single instance |

4. In **Advanced Configuration**, set **Task** to `custom`
5. Click **"Create Endpoint"**

### Cost Estimate (2025)

| GPU | Per Hour | Monthly (24/7) | Monthly (2h/day) |
|-----|----------|----------------|------------------|
| L4 | ~$0.80 | ~$576 | ~$48 |
| A10G | ~$1.10 | ~$792 | ~$66 |
| T4 | ~$0.50 | ~$360 | âŒ May run out of VRAM |

ğŸ’¡ **Min Replicas = 0**: No charge when idle
âš ï¸ Cold start takes **1-3 minutes** (model loading time)

### Troubleshooting

**Error: `PackageNotFoundError: bitsandbytes`**
- The handler automatically falls back to bfloat16 if bitsandbytes is unavailable
- Solution: Use `nvidia-l4` or `nvidia-a10g` instance

**Error: CUDA out of memory**
- Solution: Use larger instance (`nvidia-a10g` or higher) or ensure `USE_4BIT=true`

## Training Details

- **Epochs**: 3
- **Batch Size**: 1 (with gradient accumulation of 4)
- **Learning Rate**: 2e-4
- **LoRA Rank**: 16
- **LoRA Alpha**: 32

## License

Apache 2.0

---

# PDFme ãƒ•ã‚©ãƒ¼ãƒ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ¤œå‡ºãƒ¢ãƒ‡ãƒ«

**æ—¥æœ¬ã®æ›¸é¡ã‹ã‚‰ã€ç”³è«‹è€…ãŒè¨˜å…¥ã™ã¹ããƒ•ã‚©ãƒ¼ãƒ æ¬„ã‚’è‡ªå‹•æ¤œå‡ºã™ã‚‹ãƒ¢ãƒ‡ãƒ«**

[Qwen3-VL-8B-Instruct](https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct)ã‚’QLoRAã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€ç”³è«‹æ›¸ã‚„å±Šå‡ºæ›¸ãªã©ã®å…¥åŠ›æ¬„ã‚’æ¤œå‡ºã—ã¾ã™ã€‚

## ã“ã®ãƒ¢ãƒ‡ãƒ«ã§ã§ãã‚‹ã“ã¨

æ›¸é¡ã®ç”»åƒã‚’å…¥åŠ›ã™ã‚‹ã¨ã€**ç”³è«‹è€…ï¼ˆé¡§å®¢ï¼‰ãŒè¨˜å…¥ã™ã¹ãæ¬„**ã®ä½ç½®ï¼ˆbboxï¼‰ã‚’æ¤œå‡ºã—ã¾ã™ã€‚
**è·å“¡ãŒè¨˜å…¥ã™ã‚‹æ¬„**ï¼ˆå—ä»˜ç•ªå·ã€å‡¦ç†æ—¥ãªã©ï¼‰ã¯é™¤å¤–ã•ã‚Œã¾ã™ã€‚

### æ´»ç”¨ä¾‹

- ãƒ•ã‚©ãƒ¼ãƒ ã®ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–è‡ªå‹•åŒ–
- PDFå¸³ç¥¨ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 
- æ›¸é¡å‡¦ç†ã®ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£å‘ä¸Š

## ãƒ¢ãƒ‡ãƒ«æƒ…å ±

| é …ç›® | å†…å®¹ |
|------|------|
| ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ« | Qwen/Qwen3-VL-8B-Instruct |
| å­¦ç¿’æ‰‹æ³• | QLoRAï¼ˆ4bité‡å­åŒ– + LoRAï¼‰ |
| å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ | [hand-dot/pdfme-form-field-dataset](https://huggingface.co/datasets/hand-dot/pdfme-form-field-dataset) |
| å‡ºåŠ›å½¢å¼ | JSONï¼ˆ0-1000æ­£è¦åŒ–ã•ã‚ŒãŸbboxåº§æ¨™ï¼‰ |

## æ€§èƒ½è©•ä¾¡

### å­¦ç¿’ã®é€²æ—

| Epoch | Loss | å­¦ç¿’ç‡ |
|-------|------|--------|
| 0.4 | 20.74 | 0 |
| 1.0 | 20.84 | 0.000175 |
| 2.0 | 15.05 | 0.0001 |
| 2.8 | 12.24 | 0.00005 |

**Lossæ”¹å–„: 20.74 â†’ 12.24ï¼ˆ41%æ¸›å°‘ï¼‰**

### ç¾åœ¨ã®åˆ¶é™äº‹é …

1. **å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒå°‘é‡ï¼ˆ10ä»¶ï¼‰** - æ±åŒ–æ€§èƒ½ã«é™ç•Œ
2. **bboxåº§æ¨™ã®ç²¾åº¦** - æ­£è¦åŒ–ã®ãŸã‚ç”»åƒã‚µã‚¤ã‚ºã«ä¾å­˜
3. **è¤‡é›‘ãªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ** - å¤šæ®µçµ„ã¿ã‚„è¤‡é›‘ãªæ›¸é¡ã§ã¯æ¤œå‡ºæ¼ã‚Œã®å¯èƒ½æ€§

### è©•ä¾¡çµæœ

10ä»¶ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡ã—ãŸçµæœï¼š

| æŒ‡æ¨™ | å€¤ | èª¬æ˜ |
|------|-----|------|
| **Recall (æ¤œå‡ºç‡)** | 0% | IoUâ‰¥0.5ã§ãƒãƒƒãƒã—ãŸæ­£è§£ã®å‰²åˆ |
| **Precision (é©åˆç‡)** | 0% | IoUâ‰¥0.5ã§ãƒãƒƒãƒã—ãŸäºˆæ¸¬ã®å‰²åˆ |
| **å¹³å‡IoU** | 0.08 | äºˆæ¸¬ã¨æ­£è§£ã®bboxé‡ãªã‚Šåº¦åˆã„ |

âš ï¸ **ç¾çŠ¶ã®èª²é¡Œ**: å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒ10ä»¶ã¨å°‘ãªãã€ãƒ¢ãƒ‡ãƒ«ã¯ãƒ•ã‚©ãƒ¼ãƒ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ã€Œå­˜åœ¨ã€ã¯æ¤œå‡ºã§ãã‚‹ãŒã€**ä½ç½®ç²¾åº¦ãŒä½ã„**çŠ¶æ…‹ã§ã™ã€‚

### è©•ä¾¡æŒ‡æ¨™ã®å®šç¾©

- **Recallï¼ˆæ¤œå‡ºç‡ï¼‰**: æ­£è§£ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ã†ã¡ã€IoUâ‰¥0.5ã§æ¤œå‡ºã§ããŸå‰²åˆ
- **Precisionï¼ˆé©åˆç‡ï¼‰**: æ¤œå‡ºã—ãŸãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ã†ã¡ã€IoUâ‰¥0.5ã§æ­£è§£ã ã£ãŸå‰²åˆ
- **IoU**: äºˆæ¸¬bboxã¨æ­£è§£bboxã®é‡ãªã‚Šå…·åˆï¼ˆ0ã€œ1ï¼‰

## ä½¿ã„æ–¹

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
pip install transformers peft torch accelerate bitsandbytes
```

### æ¨è«–ã‚³ãƒ¼ãƒ‰

```python
import torch
from PIL import Image
from transformers import AutoProcessor, AutoModelForImageTextToText
from peft import PeftModel

# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
base_model = "Qwen/Qwen3-VL-8B-Instruct"
model = AutoModelForImageTextToText.from_pretrained(
    base_model,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True,
)
model = PeftModel.from_pretrained(model, "takumi123xxx/pdfme-form-field-detector-lora")
processor = AutoProcessor.from_pretrained(base_model, trust_remote_code=True)

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæº–å‚™
system_prompt = """ã‚ãªãŸã¯æ—¥æœ¬ã®æ›¸é¡ã‚’åˆ†æã™ã‚‹ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚
æ›¸é¡ã«ã¯2ç¨®é¡ã®å…¥åŠ›æ¬„ãŒã‚ã‚Šã¾ã™ï¼š
1. æ‹…å½“è€…ï¼ˆç”³è«‹è€…ãƒ»é¡§å®¢ï¼‰ãŒè¨˜å…¥ã™ã‚‹æ¬„ â†’ æ¤œå‡ºå¯¾è±¡
2. è·å“¡ï¼ˆå½¹æ‰€ãƒ»ä¼šç¤¾ã®æ‹…å½“è€…ï¼‰ãŒè¨˜å…¥ã™ã‚‹æ¬„ â†’ å¯¾è±¡å¤–"""

user_prompt = """ã“ã®ç”»åƒã‹ã‚‰ã€æ‹…å½“è€…ãŒè¨˜å…¥ã™ã‚‹å…¥åŠ›ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ä½ç½®ã‚’ã™ã¹ã¦æ¤œå‡ºã—ã¦ãã ã•ã„ã€‚
è·å“¡ãŒè¨˜å…¥ã™ã‚‹æ¬„ã¯é™¤å¤–ã—ã¦ãã ã•ã„ã€‚
çµæœã¯JSONå½¢å¼ã§ã€å„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®bboxåº§æ¨™ï¼ˆ0-1000æ­£è¦åŒ–ï¼‰ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚"""

# ç”»åƒèª­ã¿è¾¼ã¿
image = Image.open("æ›¸é¡.png").convert("RGB")

messages = [
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": [
        {"type": "image", "image": image},
        {"type": "text", "text": user_prompt},
    ]},
]

text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
inputs = processor(text=text, images=image, return_tensors="pt").to(model.device)

output = model.generate(**inputs, max_new_tokens=2048)
result = processor.decode(output[0], skip_special_tokens=True)
print(result)
```

### å‡ºåŠ›ä¾‹

```json
{
  "applicant_fields": [
    {"bbox": [100, 200, 500, 250]},
    {"bbox": [100, 300, 500, 350]}
  ],
  "count": 2
}
```

- `bbox`: `[x1, y1, x2, y2]` ã¯0-1000ã«æ­£è¦åŒ–ã•ã‚ŒãŸåº§æ¨™
- ãƒ”ã‚¯ã‚»ãƒ«åº§æ¨™ã¸ã®å¤‰æ›: `ãƒ”ã‚¯ã‚»ãƒ«X = bbox_x / 1000 * ç”»åƒå¹…`

## ä»Šå¾Œã®æ”¹å–„æ¡ˆ

### çŸ­æœŸçš„æ”¹å–„ï¼ˆã™ãã«å®Ÿæ–½å¯èƒ½ï¼‰

1. **ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ** - å›è»¢ã€ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›ã€ãƒã‚¤ã‚ºè¿½åŠ ã§å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’å¢—ã‚„ã™
2. **ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´** - ã‚¨ãƒãƒƒã‚¯æ•°å¢—åŠ ã€å­¦ç¿’ç‡æœ€é©åŒ–
3. **è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰** - IoUã€Precisionã€Recallã®è‡ªå‹•è¨ˆç®—

### ä¸­æœŸçš„æ”¹å–„ï¼ˆ1-2é€±é–“ï¼‰

4. **ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ç¨®é¡ã®åˆ†é¡** - æ°åã€ä½æ‰€ã€æ—¥ä»˜ãªã©ã®ç¨®é¡ã‚’è­˜åˆ¥
5. **Multi-turnå¯¾è©±å¯¾å¿œ** - ã€Œæ°åæ¬„ã ã‘æ¤œå‡ºã—ã¦ã€ãªã©ã®æ¡ä»¶ä»˜ãæ¤œå‡º
6. **ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«** - è¤‡æ•°ã®LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’å­¦ç¿’ã—ã€æŠ•ç¥¨ã§æ±ºå®š

### é•·æœŸçš„æ”¹å–„ï¼ˆ1ãƒ¶æœˆä»¥ä¸Šï¼‰

7. **å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰** - 1000ä»¶ä»¥ä¸Šã®ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿
8. **ã‚ˆã‚Šå¤§ããªãƒ¢ãƒ‡ãƒ«** - Qwen3-VL-72Bç­‰ã€PEFTå¯¾å¿œå¾Œã«è©¦è¡Œ
9. **Active Learning** - äººé–“ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼â†’ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯â†’ç¶™ç¶šçš„æ”¹å–„

## ãƒ‡ãƒ—ãƒ­ã‚¤ï¼ˆInference Endpointsï¼‰

### âš ï¸ é‡è¦ï¼šã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹é¸æŠã«ã¤ã„ã¦

ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯**8Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**ã®Vision-Language Modelã§ã™ã€‚ãƒ‡ãƒ—ãƒ­ã‚¤æ™‚ã¯ä»¥ä¸‹ã®ç‚¹ã«æ³¨æ„ã—ã¦ãã ã•ã„ï¼š

| æ¡ä»¶ | æ¨å¥¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ | VRAM | å‚™è€ƒ |
|------|-----------------|------|------|
| **4bité‡å­åŒ–ã‚ã‚Š** | `nvidia-l4` ã¾ãŸã¯ `nvidia-a10g` | 24GB | â­æ¨å¥¨ |
| **4bité‡å­åŒ–ãªã—** | `nvidia-a10g` ä»¥ä¸Š | 24GB+ | VRAMã«ä½™è£•ãŒå¿…è¦ |

**ğŸ¯ æ¨å¥¨æ§‹æˆ: `nvidia-l4` Ã— 1ï¼ˆ4bité‡å­åŒ–ï¼‰**

### ãƒ‡ãƒ—ãƒ­ã‚¤æ‰‹é †

1. [takumi123xxx/pdfme-form-field-detector-lora](https://huggingface.co/takumi123xxx/pdfme-form-field-detector-lora) ã«ã‚¢ã‚¯ã‚»ã‚¹
2. **ã€ŒDeployã€â†’ã€ŒInference Endpointsã€**ã‚’é¸æŠ
3. è¨­å®šï¼š

| é …ç›® | è¨­å®šå€¤ | èª¬æ˜ |
|------|--------|------|
| **Cloud Provider** | `AWS` | æ¨å¥¨ |
| **Region** | `us-east-1` ã¾ãŸã¯ `eu-west-1` | L4åˆ©ç”¨å¯èƒ½åœ°åŸŸ |
| **Instance Type** | â­ **`nvidia-l4`** | æœ€ã‚‚æ¨å¥¨ |
| **Instance Size** | `x1` | 1GPU |
| **Min Replicas** | `0` | ä½¿ã‚ãªã„ã¨ãã¯èª²é‡‘ãªã— |
| **Max Replicas** | `1` | å˜ä¸€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ |

4. **Advanced Configuration**ã§**Task**ã‚’`custom`ã«è¨­å®š
5. **ã€ŒCreate Endpointã€**ã‚’ã‚¯ãƒªãƒƒã‚¯

### ã‚³ã‚¹ãƒˆç›®å®‰ï¼ˆ2025å¹´æ™‚ç‚¹ï¼‰

| GPU | 1æ™‚é–“ã‚ãŸã‚Š | æœˆé¡ï¼ˆ24æ™‚é–“ï¼‰ | æœˆé¡ï¼ˆ1æ—¥2æ™‚é–“ä½¿ç”¨ï¼‰ |
|-----|------------|---------------|---------------------|
| L4 | ~$0.80 | ~$576 | ~$48 |
| A10G | ~$1.10 | ~$792 | ~$66 |
| T4 | ~$0.50 | ~$360 | âŒ VRAMä¸è¶³ã®å¯èƒ½æ€§ |

ğŸ’¡ **Min Replicas = 0**: ä½¿ã‚ãªã„ã¨ãã¯èª²é‡‘ãªã—
âš ï¸ Cold Startã«**1-3åˆ†**ã‹ã‹ã‚Šã¾ã™ï¼ˆãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æ™‚é–“ï¼‰

### ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

**ã‚¨ãƒ©ãƒ¼: `PackageNotFoundError: bitsandbytes`**
- handler.pyã¯è‡ªå‹•çš„ã«bfloat16ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™
- è§£æ±ºç­–: `nvidia-l4`ã¾ãŸã¯`nvidia-a10g`ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’é¸æŠ

**ã‚¨ãƒ©ãƒ¼: CUDA out of memory**
- è§£æ±ºç­–: ã‚ˆã‚Šå¤§ããªã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆ`nvidia-a10g`ä»¥ä¸Šï¼‰ã‚’é¸æŠã€ã¾ãŸã¯`USE_4BIT=true`ã‚’ç¢ºèª

## å­¦ç¿’è©³ç´°

- **ã‚¨ãƒãƒƒã‚¯æ•°**: 3
- **ãƒãƒƒãƒã‚µã‚¤ã‚º**: 1ï¼ˆå‹¾é…ç´¯ç©: 4ï¼‰
- **å­¦ç¿’ç‡**: 2e-4
- **LoRAãƒ©ãƒ³ã‚¯**: 16
- **LoRAã‚¢ãƒ«ãƒ•ã‚¡**: 32

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

Apache 2.0
