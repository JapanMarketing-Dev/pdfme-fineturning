---
license: apache-2.0
language:
  - ja
  - en
library_name: transformers
pipeline_tag: image-text-to-text
tags:
  - vision
  - vlm
  - qwen
  - lora
  - document-understanding
  - form-detection
  - japanese
base_model: Qwen/Qwen3-VL-8B-Instruct
datasets:
  - hand-dot/pdfme-form-field-dataset
---

# PDFme Form Field Detector

**Detects form fields that applicants need to fill in Japanese documents.**

This model is fine-tuned from [Qwen3-VL-8B-Instruct](https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct) using QLoRA to detect input fields in Japanese application forms, registration documents, and other official paperwork.

## What This Model Does

Given an image of a Japanese document, this model identifies the bounding boxes of form fields that **applicants/customers** should fill in, while **excluding fields meant for staff/officials**.

### Example Use Cases

- Automating form digitization
- Building PDF form generators
- Creating accessibility tools for document processing

## Model Details

| Item | Value |
|------|-------|
| Base Model | Qwen/Qwen3-VL-8B-Instruct |
| Fine-tuning Method | QLoRA (4-bit quantization + LoRA) |
| Training Data | [hand-dot/pdfme-form-field-dataset](https://huggingface.co/datasets/hand-dot/pdfme-form-field-dataset) |
| Output Format | JSON with normalized bbox coordinates (0-1000) |

## Performance

### Training Progress

| Epoch | Loss | Learning Rate |
|-------|------|---------------|
| 0.4 | 20.74 | 0 |
| 1.0 | 20.84 | 0.000175 |
| 2.0 | 15.05 | 0.0001 |
| 2.8 | 12.24 | 0.00005 |

**Loss improved: 20.74 â†’ 12.24 (41% reduction)**

### Current Limitations

1. **Small training dataset (10 samples)** - Limited generalization
2. **Bbox coordinate precision** - Depends on image size due to normalization
3. **Complex layouts** - May miss fields in multi-column or complex documents

### Metrics (To be measured)

- **Recall**: Percentage of ground truth fields detected
- **Precision**: Percentage of detected fields that are correct
- **IoU**: Overlap between predicted and ground truth bboxes

## Quick Start

### Installation

```bash
pip install transformers peft torch accelerate bitsandbytes
```

### Inference

```python
import torch
from PIL import Image
from transformers import AutoProcessor, AutoModelForImageTextToText
from peft import PeftModel

# Load model
base_model = "Qwen/Qwen3-VL-8B-Instruct"
model = AutoModelForImageTextToText.from_pretrained(
    base_model,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True,
)
model = PeftModel.from_pretrained(model, "takumi123xxx/pdfme-form-field-detector-lora")
processor = AutoProcessor.from_pretrained(base_model, trust_remote_code=True)

# Prepare prompt
system_prompt = """You are an expert at analyzing Japanese documents.
There are two types of input fields:
1. Fields for applicants/customers to fill â†’ Target for detection
2. Fields for staff/officials to fill â†’ Exclude from detection"""

user_prompt = """Detect all input fields that applicants should fill in this image.
Exclude fields for staff.
Return JSON with bbox coordinates (0-1000 normalized)."""

# Load image
image = Image.open("your_document.png").convert("RGB")

messages = [
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": [
        {"type": "image", "image": image},
        {"type": "text", "text": user_prompt},
    ]},
]

text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
inputs = processor(text=text, images=image, return_tensors="pt").to(model.device)

output = model.generate(**inputs, max_new_tokens=2048)
result = processor.decode(output[0], skip_special_tokens=True)
print(result)
```

### Output Format

```json
{
  "applicant_fields": [
    {"bbox": [100, 200, 500, 250]},
    {"bbox": [100, 300, 500, 350]}
  ],
  "count": 2
}
```

- `bbox`: `[x1, y1, x2, y2]` normalized to 0-1000 scale
- To convert to pixels: `pixel_x = bbox_x / 1000 * image_width`

## Future Improvements

### Short-term

1. **Data augmentation** - Rotation, scaling, noise to expand training data
2. **Hyperparameter tuning** - Increase epochs, adjust learning rate
3. **Evaluation pipeline** - Automated IoU, Precision, Recall calculation

### Mid-term

4. **Field type classification** - Identify field types (name, address, date, etc.)
5. **Multi-turn dialogue** - Support conditional detection ("only detect name fields")
6. **Model ensemble** - Train multiple LoRA adapters and vote

### Long-term

7. **Large-scale dataset** - 1000+ annotated samples across document types
8. **Larger models** - Qwen3-VL-72B when PEFT compatible
9. **Active learning** - Human review â†’ feedback â†’ continuous improvement

## Deployment (Inference Endpoints)

### Recommended GPU

| GPU | VRAM | Recommendation | Reason |
|-----|------|----------------|--------|
| **NVIDIA L4** | 24GB | â­â­â­ | Best cost-performance, works well with 4-bit |
| **NVIDIA A10G** | 24GB | â­â­â­ | Stable, common on AWS |
| **NVIDIA T4** | 16GB | â­â­ | Cheap but tight, slower inference |
| **NVIDIA A100** | 40GB+ | â­ | Overkill, expensive |

**Recommendation: NVIDIA L4 Ã— 1**

### Cost Estimate (2025)

| GPU | Per Hour | Monthly (24/7) |
|-----|----------|----------------|
| T4 | ~$0.50 | ~$360 |
| L4 | ~$0.80 | ~$576 |
| A10G | ~$1.10 | ~$792 |

ğŸ’¡ Set **Min Replicas = 0** to avoid charges when idle (30s-1min cold start)

### Deployment Settings

| Setting | Recommended Value |
|---------|-------------------|
| **Cloud Provider** | AWS or GCP |
| **Region** | `ap-northeast-1` (Tokyo) or nearest |
| **Instance Type** | `GPU - L4` or `GPU - A10G` |
| **Instance Size** | `x1` (1 GPU) |
| **Min Replicas** | `0` (cost saving) |
| **Max Replicas** | `1` |

## Training Details

- **Epochs**: 3
- **Batch Size**: 1 (with gradient accumulation of 4)
- **Learning Rate**: 2e-4
- **LoRA Rank**: 16
- **LoRA Alpha**: 32

## License

Apache 2.0

---

# PDFme ãƒ•ã‚©ãƒ¼ãƒ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ¤œå‡ºãƒ¢ãƒ‡ãƒ«

**æ—¥æœ¬ã®æ›¸é¡ã‹ã‚‰ã€ç”³è«‹è€…ãŒè¨˜å…¥ã™ã¹ããƒ•ã‚©ãƒ¼ãƒ æ¬„ã‚’è‡ªå‹•æ¤œå‡ºã™ã‚‹ãƒ¢ãƒ‡ãƒ«**

[Qwen3-VL-8B-Instruct](https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct)ã‚’QLoRAã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€ç”³è«‹æ›¸ã‚„å±Šå‡ºæ›¸ãªã©ã®å…¥åŠ›æ¬„ã‚’æ¤œå‡ºã—ã¾ã™ã€‚

## ã“ã®ãƒ¢ãƒ‡ãƒ«ã§ã§ãã‚‹ã“ã¨

æ›¸é¡ã®ç”»åƒã‚’å…¥åŠ›ã™ã‚‹ã¨ã€**ç”³è«‹è€…ï¼ˆé¡§å®¢ï¼‰ãŒè¨˜å…¥ã™ã¹ãæ¬„**ã®ä½ç½®ï¼ˆbboxï¼‰ã‚’æ¤œå‡ºã—ã¾ã™ã€‚
**è·å“¡ãŒè¨˜å…¥ã™ã‚‹æ¬„**ï¼ˆå—ä»˜ç•ªå·ã€å‡¦ç†æ—¥ãªã©ï¼‰ã¯é™¤å¤–ã•ã‚Œã¾ã™ã€‚

### æ´»ç”¨ä¾‹

- ãƒ•ã‚©ãƒ¼ãƒ ã®ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–è‡ªå‹•åŒ–
- PDFå¸³ç¥¨ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 
- æ›¸é¡å‡¦ç†ã®ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£å‘ä¸Š

## ãƒ¢ãƒ‡ãƒ«æƒ…å ±

| é …ç›® | å†…å®¹ |
|------|------|
| ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ« | Qwen/Qwen3-VL-8B-Instruct |
| å­¦ç¿’æ‰‹æ³• | QLoRAï¼ˆ4bité‡å­åŒ– + LoRAï¼‰ |
| å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ | [hand-dot/pdfme-form-field-dataset](https://huggingface.co/datasets/hand-dot/pdfme-form-field-dataset) |
| å‡ºåŠ›å½¢å¼ | JSONï¼ˆ0-1000æ­£è¦åŒ–ã•ã‚ŒãŸbboxåº§æ¨™ï¼‰ |

## æ€§èƒ½è©•ä¾¡

### å­¦ç¿’ã®é€²æ—

| Epoch | Loss | å­¦ç¿’ç‡ |
|-------|------|--------|
| 0.4 | 20.74 | 0 |
| 1.0 | 20.84 | 0.000175 |
| 2.0 | 15.05 | 0.0001 |
| 2.8 | 12.24 | 0.00005 |

**Lossæ”¹å–„: 20.74 â†’ 12.24ï¼ˆ41%æ¸›å°‘ï¼‰**

### ç¾åœ¨ã®åˆ¶é™äº‹é …

1. **å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒå°‘é‡ï¼ˆ10ä»¶ï¼‰** - æ±åŒ–æ€§èƒ½ã«é™ç•Œ
2. **bboxåº§æ¨™ã®ç²¾åº¦** - æ­£è¦åŒ–ã®ãŸã‚ç”»åƒã‚µã‚¤ã‚ºã«ä¾å­˜
3. **è¤‡é›‘ãªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ** - å¤šæ®µçµ„ã¿ã‚„è¤‡é›‘ãªæ›¸é¡ã§ã¯æ¤œå‡ºæ¼ã‚Œã®å¯èƒ½æ€§

### è©•ä¾¡æŒ‡æ¨™ï¼ˆä»Šå¾Œæ¸¬å®šäºˆå®šï¼‰

- **æ¤œå‡ºç‡ï¼ˆRecallï¼‰**: æ­£è§£ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ã†ã¡ã€æ¤œå‡ºã§ããŸå‰²åˆ
- **é©åˆç‡ï¼ˆPrecisionï¼‰**: æ¤œå‡ºã—ãŸãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ã†ã¡ã€æ­£è§£ã ã£ãŸå‰²åˆ
- **IoU**: bboxåº§æ¨™ã®é‡ãªã‚Šå…·åˆ

## ä½¿ã„æ–¹

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
pip install transformers peft torch accelerate bitsandbytes
```

### æ¨è«–ã‚³ãƒ¼ãƒ‰

```python
import torch
from PIL import Image
from transformers import AutoProcessor, AutoModelForImageTextToText
from peft import PeftModel

# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
base_model = "Qwen/Qwen3-VL-8B-Instruct"
model = AutoModelForImageTextToText.from_pretrained(
    base_model,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True,
)
model = PeftModel.from_pretrained(model, "takumi123xxx/pdfme-form-field-detector-lora")
processor = AutoProcessor.from_pretrained(base_model, trust_remote_code=True)

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæº–å‚™
system_prompt = """ã‚ãªãŸã¯æ—¥æœ¬ã®æ›¸é¡ã‚’åˆ†æã™ã‚‹ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚
æ›¸é¡ã«ã¯2ç¨®é¡ã®å…¥åŠ›æ¬„ãŒã‚ã‚Šã¾ã™ï¼š
1. æ‹…å½“è€…ï¼ˆç”³è«‹è€…ãƒ»é¡§å®¢ï¼‰ãŒè¨˜å…¥ã™ã‚‹æ¬„ â†’ æ¤œå‡ºå¯¾è±¡
2. è·å“¡ï¼ˆå½¹æ‰€ãƒ»ä¼šç¤¾ã®æ‹…å½“è€…ï¼‰ãŒè¨˜å…¥ã™ã‚‹æ¬„ â†’ å¯¾è±¡å¤–"""

user_prompt = """ã“ã®ç”»åƒã‹ã‚‰ã€æ‹…å½“è€…ãŒè¨˜å…¥ã™ã‚‹å…¥åŠ›ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ä½ç½®ã‚’ã™ã¹ã¦æ¤œå‡ºã—ã¦ãã ã•ã„ã€‚
è·å“¡ãŒè¨˜å…¥ã™ã‚‹æ¬„ã¯é™¤å¤–ã—ã¦ãã ã•ã„ã€‚
çµæœã¯JSONå½¢å¼ã§ã€å„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®bboxåº§æ¨™ï¼ˆ0-1000æ­£è¦åŒ–ï¼‰ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚"""

# ç”»åƒèª­ã¿è¾¼ã¿
image = Image.open("æ›¸é¡.png").convert("RGB")

messages = [
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": [
        {"type": "image", "image": image},
        {"type": "text", "text": user_prompt},
    ]},
]

text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
inputs = processor(text=text, images=image, return_tensors="pt").to(model.device)

output = model.generate(**inputs, max_new_tokens=2048)
result = processor.decode(output[0], skip_special_tokens=True)
print(result)
```

### å‡ºåŠ›ä¾‹

```json
{
  "applicant_fields": [
    {"bbox": [100, 200, 500, 250]},
    {"bbox": [100, 300, 500, 350]}
  ],
  "count": 2
}
```

- `bbox`: `[x1, y1, x2, y2]` ã¯0-1000ã«æ­£è¦åŒ–ã•ã‚ŒãŸåº§æ¨™
- ãƒ”ã‚¯ã‚»ãƒ«åº§æ¨™ã¸ã®å¤‰æ›: `ãƒ”ã‚¯ã‚»ãƒ«X = bbox_x / 1000 * ç”»åƒå¹…`

## ä»Šå¾Œã®æ”¹å–„æ¡ˆ

### çŸ­æœŸçš„æ”¹å–„ï¼ˆã™ãã«å®Ÿæ–½å¯èƒ½ï¼‰

1. **ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ** - å›è»¢ã€ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›ã€ãƒã‚¤ã‚ºè¿½åŠ ã§å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’å¢—ã‚„ã™
2. **ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´** - ã‚¨ãƒãƒƒã‚¯æ•°å¢—åŠ ã€å­¦ç¿’ç‡æœ€é©åŒ–
3. **è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰** - IoUã€Precisionã€Recallã®è‡ªå‹•è¨ˆç®—

### ä¸­æœŸçš„æ”¹å–„ï¼ˆ1-2é€±é–“ï¼‰

4. **ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ç¨®é¡ã®åˆ†é¡** - æ°åã€ä½æ‰€ã€æ—¥ä»˜ãªã©ã®ç¨®é¡ã‚’è­˜åˆ¥
5. **Multi-turnå¯¾è©±å¯¾å¿œ** - ã€Œæ°åæ¬„ã ã‘æ¤œå‡ºã—ã¦ã€ãªã©ã®æ¡ä»¶ä»˜ãæ¤œå‡º
6. **ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«** - è¤‡æ•°ã®LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’å­¦ç¿’ã—ã€æŠ•ç¥¨ã§æ±ºå®š

### é•·æœŸçš„æ”¹å–„ï¼ˆ1ãƒ¶æœˆä»¥ä¸Šï¼‰

7. **å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰** - 1000ä»¶ä»¥ä¸Šã®ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿
8. **ã‚ˆã‚Šå¤§ããªãƒ¢ãƒ‡ãƒ«** - Qwen3-VL-72Bç­‰ã€PEFTå¯¾å¿œå¾Œã«è©¦è¡Œ
9. **Active Learning** - äººé–“ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼â†’ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯â†’ç¶™ç¶šçš„æ”¹å–„

## ãƒ‡ãƒ—ãƒ­ã‚¤ï¼ˆInference Endpointsï¼‰

### æ¨å¥¨GPU

| GPU | VRAM | æ¨å¥¨åº¦ | ç†ç”± |
|-----|------|--------|------|
| **NVIDIA L4** | 24GB | â­â­â­ | ã‚³ã‚¹ãƒ‘æœ€é«˜ã€4bité‡å­åŒ–ã§ååˆ†å‹•ä½œ |
| **NVIDIA A10G** | 24GB | â­â­â­ | å®‰å®šã€AWSã§ä¸€èˆ¬çš„ |
| **NVIDIA T4** | 16GB | â­â­ | å®‰ã„ãŒã‚®ãƒªã‚®ãƒªã€æ¨è«–é€Ÿåº¦é…ã‚ |
| **NVIDIA A100** | 40GB+ | â­ | ã‚ªãƒ¼ãƒãƒ¼ã‚¹ãƒšãƒƒã‚¯ã€é«˜ã‚³ã‚¹ãƒˆ |

**æ¨å¥¨: NVIDIA L4 Ã— 1**

### ã‚³ã‚¹ãƒˆç›®å®‰ï¼ˆ2025å¹´æ™‚ç‚¹ï¼‰

| GPU | 1æ™‚é–“ã‚ãŸã‚Š | æœˆé¡ï¼ˆ24æ™‚é–“ç¨¼åƒï¼‰ |
|-----|------------|-------------------|
| T4 | ~$0.50 | ~$360 |
| L4 | ~$0.80 | ~$576 |
| A10G | ~$1.10 | ~$792 |

ğŸ’¡ **Min Replicas = 0** ã«è¨­å®šã™ã‚‹ã¨ã€ä½¿ã‚ãªã„ã¨ãã¯èª²é‡‘ã•ã‚Œã¾ã›ã‚“ï¼ˆCold Startæ™‚ã«30ç§’ã€œ1åˆ†ã‹ã‹ã‚‹ï¼‰

### ãƒ‡ãƒ—ãƒ­ã‚¤è¨­å®š

| é …ç›® | æ¨å¥¨å€¤ |
|------|--------|
| **Cloud Provider** | AWS ã¾ãŸã¯ GCP |
| **Region** | `ap-northeast-1`ï¼ˆæ±äº¬ï¼‰ã‹è¿‘ã„åœ°åŸŸ |
| **Instance Type** | `GPU - L4` ã¾ãŸã¯ `GPU - A10G` |
| **Instance Size** | `x1`ï¼ˆ1GPUï¼‰ |
| **Min Replicas** | `0`ï¼ˆã‚³ã‚¹ãƒˆç¯€ç´„ï¼‰ |
| **Max Replicas** | `1` |

## å­¦ç¿’è©³ç´°

- **ã‚¨ãƒãƒƒã‚¯æ•°**: 3
- **ãƒãƒƒãƒã‚µã‚¤ã‚º**: 1ï¼ˆå‹¾é…ç´¯ç©: 4ï¼‰
- **å­¦ç¿’ç‡**: 2e-4
- **LoRAãƒ©ãƒ³ã‚¯**: 16
- **LoRAã‚¢ãƒ«ãƒ•ã‚¡**: 32

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

Apache 2.0
